{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441322b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 104.jpg: JSON annotation not found.\n",
      "Skipping 109.jpg: JSON annotation not found.\n",
      "Skipping 11.jpg: JSON annotation not found.\n",
      "Skipping 117.jpg: JSON annotation not found.\n",
      "Skipping 118.jpg: JSON annotation not found.\n",
      "Skipping 126.jpg: JSON annotation not found.\n",
      "Skipping 132.jpg: JSON annotation not found.\n",
      "Skipping 135.jpg: JSON annotation not found.\n",
      "Skipping 139.jpg: JSON annotation not found.\n",
      "Skipping 140.jpg: JSON annotation not found.\n",
      "Skipping 141.jpg: JSON annotation not found.\n",
      "Skipping 146.jpg: JSON annotation not found.\n",
      "Skipping 148.jpg: JSON annotation not found.\n",
      "Skipping 155.jpg: JSON annotation not found.\n",
      "Skipping 158.jpg: JSON annotation not found.\n",
      "Skipping 161.jpg: JSON annotation not found.\n",
      "Skipping 17.jpg: JSON annotation not found.\n",
      "Skipping 180.jpg: JSON annotation not found.\n",
      "Skipping 184.jpg: JSON annotation not found.\n",
      "Skipping 185.jpg: JSON annotation not found.\n",
      "Skipping 186.jpg: JSON annotation not found.\n",
      "Skipping 187.jpg: JSON annotation not found.\n",
      "Skipping 190.jpg: JSON annotation not found.\n",
      "Skipping 2.jpg: JSON annotation not found.\n",
      "Skipping 200.jpg: JSON annotation not found.\n",
      "Skipping 201.jpg: JSON annotation not found.\n",
      "Skipping 209.jpg: JSON annotation not found.\n",
      "Skipping 21.jpg: JSON annotation not found.\n",
      "Skipping 210.jpg: JSON annotation not found.\n",
      "Skipping 211.jpg: JSON annotation not found.\n",
      "Skipping 213.jpg: JSON annotation not found.\n",
      "Skipping 216.jpg: JSON annotation not found.\n",
      "Skipping 22.jpg: JSON annotation not found.\n",
      "Skipping 23.jpg: JSON annotation not found.\n",
      "Skipping 236.jpg: JSON annotation not found.\n",
      "Skipping 24.jpg: JSON annotation not found.\n",
      "Skipping 244.jpg: JSON annotation not found.\n",
      "Skipping 246.jpg: JSON annotation not found.\n",
      "Skipping 251.jpg: JSON annotation not found.\n",
      "Skipping 26.jpg: JSON annotation not found.\n",
      "Skipping 260.jpg: JSON annotation not found.\n",
      "Skipping 269.jpg: JSON annotation not found.\n",
      "Skipping 271.jpg: JSON annotation not found.\n",
      "Skipping 275.jpg: JSON annotation not found.\n",
      "Skipping 276.jpg: JSON annotation not found.\n",
      "Skipping 31.jpg: JSON annotation not found.\n",
      "Skipping 32.jpg: JSON annotation not found.\n",
      "Skipping 37.jpg: JSON annotation not found.\n",
      "Skipping 430.jpg: JSON annotation not found.\n",
      "Skipping 431.jpg: JSON annotation not found.\n",
      "Skipping 432.jpg: JSON annotation not found.\n",
      "Skipping 433.jpg: JSON annotation not found.\n",
      "Skipping 435.jpg: JSON annotation not found.\n",
      "Skipping 437.jpg: JSON annotation not found.\n",
      "Skipping 439.jpg: JSON annotation not found.\n",
      "Skipping 446.jpg: JSON annotation not found.\n",
      "Skipping 447.jpg: JSON annotation not found.\n",
      "Skipping 46.jpg: JSON annotation not found.\n",
      "Skipping 460.jpg: JSON annotation not found.\n",
      "Skipping 464.jpg: JSON annotation not found.\n",
      "Skipping 465.jpg: JSON annotation not found.\n",
      "Skipping 466.jpg: JSON annotation not found.\n",
      "Skipping 471.jpg: JSON annotation not found.\n",
      "Skipping 479.jpg: JSON annotation not found.\n",
      "Skipping 482.jpg: JSON annotation not found.\n",
      "Skipping 491.jpg: JSON annotation not found.\n",
      "Skipping 493.jpg: JSON annotation not found.\n",
      "Skipping 512.jpg: JSON annotation not found.\n",
      "Skipping 513.jpg: JSON annotation not found.\n",
      "Skipping 519.jpg: JSON annotation not found.\n",
      "Skipping 526.jpg: JSON annotation not found.\n",
      "Skipping 529.jpg: JSON annotation not found.\n",
      "Skipping 530.jpg: JSON annotation not found.\n",
      "Skipping 532.jpg: JSON annotation not found.\n",
      "Skipping 540.jpg: JSON annotation not found.\n",
      "Skipping 543.jpg: JSON annotation not found.\n",
      "Skipping 55.jpg: JSON annotation not found.\n",
      "Skipping 553.jpg: JSON annotation not found.\n",
      "Skipping 558.jpg: JSON annotation not found.\n",
      "Skipping 564.jpg: JSON annotation not found.\n",
      "Skipping 565.jpg: JSON annotation not found.\n",
      "Skipping 572.jpg: JSON annotation not found.\n",
      "Skipping 574.jpg: JSON annotation not found.\n",
      "Skipping 589.jpg: JSON annotation not found.\n",
      "Skipping 60.jpg: JSON annotation not found.\n",
      "Skipping 61.jpg: JSON annotation not found.\n",
      "Skipping 63.jpg: JSON annotation not found.\n",
      "Skipping 67.jpg: JSON annotation not found.\n",
      "Skipping 7.jpg: JSON annotation not found.\n",
      "Skipping 70.jpg: JSON annotation not found.\n",
      "Skipping 72.jpg: JSON annotation not found.\n",
      "Skipping 73.jpg: JSON annotation not found.\n",
      "Skipping 74.jpg: JSON annotation not found.\n",
      "Skipping 76.jpg: JSON annotation not found.\n",
      "Skipping 77.jpg: JSON annotation not found.\n",
      "Skipping 87.jpg: JSON annotation not found.\n",
      "Skipping 90.jpg: JSON annotation not found.\n",
      "Skipping 91.jpg: JSON annotation not found.\n",
      "Skipping 92.jpg: JSON annotation not found.\n",
      "\n",
      "All masks generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths\n",
    "image_dir = 'D:\\BTP\\Dataset\\set_btp\\set_1\\set_1'\n",
    "annotation_dir = 'D:\\BTP\\Dataset\\btp_json\\json_1'\n",
    "mask_dir = 'D:\\BTP\\Dataset\\masks\\mask_1'\n",
    "os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "# Color definitions for regions (BGR)\n",
    "colors = {\n",
    "    'heart': (0, 0, 255),    # Red\n",
    "    'liver': (0, 255, 0),    # Green\n",
    "    'stomach': (0, 255, 255) # Yellow\n",
    "}\n",
    "\n",
    "for image_file in os.listdir(image_dir):\n",
    "    if not image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        continue\n",
    "\n",
    "    basename = os.path.splitext(image_file)[0]\n",
    "    json_file = os.path.join(annotation_dir, basename + '.json')\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "\n",
    "    # Check if corresponding JSON exists\n",
    "    if not os.path.exists(json_file):\n",
    "        print(f\"Skipping {image_file}: JSON annotation not found.\")\n",
    "        continue\n",
    "\n",
    "    # Load image dimensions\n",
    "    image = Image.open(image_path)\n",
    "    img_width, img_height = image.size\n",
    "\n",
    "    # Load annotation data\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Determine original annotation dimensions\n",
    "    if 'imageWidth' in data and 'imageHeight' in data:\n",
    "        orig_width, orig_height = data['imageWidth'], data['imageHeight']\n",
    "    else:\n",
    "        # Fallback method\n",
    "        all_x = [p[0] for shape in data['shapes'] for p in shape['points']]\n",
    "        all_y = [p[1] for shape in data['shapes'] for p in shape['points']]\n",
    "        orig_width, orig_height = int(max(all_x)), int(max(all_y))\n",
    "\n",
    "    # Scaling factors\n",
    "    x_scale = img_width / orig_width\n",
    "    y_scale = img_height / orig_height\n",
    "\n",
    "    # Create empty color mask\n",
    "    mask = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Draw scaled polygons\n",
    "    for shape in data['shapes']:\n",
    "        label = shape['label']\n",
    "        points = np.array([[int(p[0]*x_scale), int(p[1]*y_scale)] for p in shape['points']], np.int32)\n",
    "        cv2.fillPoly(mask, [points], colors[label])\n",
    "\n",
    "    # Save mask image\n",
    "    mask_path = os.path.join(mask_dir, basename + '_mask.png')\n",
    "    cv2.imwrite(mask_path, mask)\n",
    "\n",
    "    print(f\"Generated mask for {image_file} at {mask_path}\")\n",
    "\n",
    "print(\"\\nAll masks generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de4fb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\compilers\\python\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: opencv-python in d:\\compilers\\python\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: Pillow in d:\\compilers\\python\\lib\\site-packages (10.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy opencv-python Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ae1d728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colored segmentation mask generated and saved at 'D:/BTP/Dataset/masks/mask_2/641_mask.png'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# File paths\n",
    "# image_path = 'D:\\BTP\\Dataset\\set_btp\\set_1\\set_1\\ 2.jpg'\n",
    "image_path = 'D:/BTP/Dataset/set_btp/set_2/set_2/641.jpg'\n",
    "json_path = 'D:/BTP/Dataset/btp_json/json_2/641.json'\n",
    "# json_path = 'D:\\BTP\\Dataset\\ btp_json\\json_1\\ 7.json'\n",
    "mask_path = 'D:/BTP/Dataset/masks/mask_2/641_mask.png'\n",
    "# mask_path = 'D:\\BTP\\Dataset\\masks\\mask_1\\ 2_mask.png'\n",
    "\n",
    "# Load image and get dimensions\n",
    "image = Image.open(image_path)\n",
    "img_width, img_height = image.size\n",
    "\n",
    "# Load JSON data\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract original annotation dimensions (if provided by annotation tool)\n",
    "if 'imageWidth' in data and 'imageHeight' in data:\n",
    "    orig_width, orig_height = data['imageWidth'], data['imageHeight']\n",
    "else:\n",
    "    # Fallback: infer dimensions from points\n",
    "    all_x = [p[0] for shape in data['shapes'] for p in shape['points']]\n",
    "    all_y = [p[1] for shape in data['shapes'] for p in shape['points']]\n",
    "    orig_width, orig_height = int(max(all_x)), int(max(all_y))\n",
    "\n",
    "# Scale coordinates\n",
    "x_scale = img_width / orig_width\n",
    "y_scale = img_height / orig_height\n",
    "\n",
    "# Prepare an empty color mask (colored according to region labels)\n",
    "color_mask = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "\n",
    "# Define colors (BGR format)\n",
    "colors = {\n",
    "    'heart': (0, 0, 255),    # Red\n",
    "    'liver': (0, 255, 0),    # Green\n",
    "    'stomach': (0, 255, 255) # Yellow\n",
    "}\n",
    "\n",
    "# Draw scaled polygons onto mask\n",
    "for shape in data['shapes']:\n",
    "    label = shape['label']\n",
    "    points = np.array([[int(p[0]*x_scale), int(p[1]*y_scale)] for p in shape['points']], dtype=np.int32)\n",
    "    cv2.fillPoly(color_mask, [points], colors[label])\n",
    "\n",
    "# Save the generated mask\n",
    "cv2.imwrite(mask_path, color_mask)\n",
    "\n",
    "print(f\"Colored segmentation mask generated and saved at '{mask_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --image_dir IMAGE_DIR --json_dir JSON_DIR\n",
      "                             --mask_dir MASK_DIR\n",
      "ipykernel_launcher.py: error: the following arguments are required: --image_dir, --json_dir, --mask_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aman Tripathi\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import argparse\n",
    "\n",
    "def generate_mask(image_path, json_path, mask_path):\n",
    "    try:\n",
    "        # Load image and get dimensions\n",
    "        image = Image.open(image_path)\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        # Load JSON data\n",
    "        with open(json_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Extract original annotation dimensions\n",
    "        if 'imageWidth' in data and 'imageHeight' in data:\n",
    "            orig_width, orig_height = data['imageWidth'], data['imageHeight']\n",
    "        else:\n",
    "            # Fallback: infer dimensions from points\n",
    "            all_x = [p[0] for shape in data['shapes'] for p in shape['points']]\n",
    "            all_y = [p[1] for shape in data['shapes'] for p in shape['points']]\n",
    "            orig_width, orig_height = int(max(all_x)) + 1, int(max(all_y)) + 1\n",
    "\n",
    "        # Scale coordinates\n",
    "        x_scale = img_width / orig_width\n",
    "        y_scale = img_height / orig_height\n",
    "\n",
    "        # Prepare an empty color mask\n",
    "        color_mask = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "\n",
    "        # Define colors (BGR format)\n",
    "        colors = {\n",
    "            'heart': (0, 0, 255),    # Red\n",
    "            'liver': (0, 255, 0),    # Green\n",
    "            'stomach': (0, 255, 255)  # Yellow\n",
    "        }\n",
    "\n",
    "        # Draw scaled polygons onto mask\n",
    "        for shape in data['shapes']:\n",
    "            label = shape['label'].lower()  # Case insensitive\n",
    "            if label not in colors:\n",
    "                print(f\"Warning: Unknown label '{label}' in {json_path}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            points = np.array([[int(p[0]*x_scale), int(p[1]*y_scale)] for p in shape['points']], dtype=np.int32)\n",
    "            cv2.fillPoly(color_mask, [points], colors[label])\n",
    "\n",
    "        # Save the generated mask\n",
    "        os.makedirs(os.path.dirname(mask_path), exist_ok=True)\n",
    "        cv2.imwrite(mask_path, color_mask)\n",
    "        print(f\"Generated mask: {mask_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_all_images(image_dir, json_dir, mask_dir):\n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # Process each image\n",
    "    success_count = 0\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for img_file in image_files:\n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "            \n",
    "            # Construct paths\n",
    "            image_path = os.path.join(image_dir, img_file)\n",
    "            json_path = os.path.join(json_dir, f\"{base_name}.json\")\n",
    "            mask_path = os.path.join(mask_dir, f\"{base_name}_mask.png\")\n",
    "            \n",
    "            # Skip if JSON doesn't exist\n",
    "            if not os.path.exists(json_path):\n",
    "                print(f\"Warning: JSON file not found for {img_file}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            futures.append(executor.submit(generate_mask, image_path, json_path, mask_path))\n",
    "        \n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            success_count += 1 if future.result() else 0\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Successfully generated {success_count}/{len(image_files)} masks.\")\n",
    "\n",
    "def main():\n",
    "    # Set up argument parser\n",
    "   \n",
    "    parser = argparse.ArgumentParser(description='Generate segmentation masks from JSON annotations')\n",
    "    parser.add_argument('--image_dir', required=True, help='Directory containing input images')\n",
    "    parser.add_argument('--json_dir', required=True, help='Directory containing JSON annotations')\n",
    "    parser.add_argument('--mask_dir', required=True, help='Output directory for masks')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    image_dir = 'D:/BTP/Dataset/set_btp/set_3/set_3'\n",
    "    json_dir = 'D:/BTP/Dataset/btp_json/json_3'\n",
    "    mask_dir = 'D:/BTP/Dataset/masks/mask_3'\n",
    "    # Process all images\n",
    "    process_all_images(\n",
    "        image_dir=args.image_dir,\n",
    "        json_dir=args.json_dir,\n",
    "        mask_dir=args.mask_dir\n",
    "    \n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1a9157b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2541_mask.pngGenerated mask: D:/BTP/Dataset/masks/mask_8\\2542_mask.png\n",
      "\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2543_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2549_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2548_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2554_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2551_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2557_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2555_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2561_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2558_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2570_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2569_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2567_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2587_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2572_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2563_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2596_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2589_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2574_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2593_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2578_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2584_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2590_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2588_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2562_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2597_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2599_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2581_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2612_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2607_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2615_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2601_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2616_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2611_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2600_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2614_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2602_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2625_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2641_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2624_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2606_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2592_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2605_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2608_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2633_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2609_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2636_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2629_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2637_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2632_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2639_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2645_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2643_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2648_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2651_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2655_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2654_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2644_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2652_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2646_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2649_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2653_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2603_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2658_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2656_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2618_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2660_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2670_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2657_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2663_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2668_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2667_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2682_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2679_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2676_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2671_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2672_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2695_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2705_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2680_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2693_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2697_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2675_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2706_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2707_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2703_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2711_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2719_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2718_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2712_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2713_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2721_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2720_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2714_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2716_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2715_mask.png\n",
      "Generated mask: D:/BTP/Dataset/masks/mask_8\\2722_mask.png\n",
      "\n",
      "Processing complete. Successfully generated 98/98 masks.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Define your specific paths\n",
    "image_dir = 'D:/BTP/Dataset/set_btp/set_8/set_8'\n",
    "json_dir = 'D:/BTP/Dataset/btp_json/json_8'\n",
    "mask_dir = 'D:/BTP/Dataset/masks/mask_8'\n",
    "\n",
    "# Define colors (BGR format)\n",
    "colors = {\n",
    "    'heart': (0, 0, 255),    # Red\n",
    "    'liver': (0, 255, 0),    # Green\n",
    "    'stomach': (0, 255, 255) # Yellow\n",
    "}\n",
    "\n",
    "def generate_mask(image_path, json_path, mask_path, colors):\n",
    "    try:\n",
    "        # Load image and get dimensions\n",
    "        image = Image.open(image_path)\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        # Load JSON data\n",
    "        with open(json_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Extract original annotation dimensions\n",
    "        if 'imageWidth' in data and 'imageHeight' in data:\n",
    "            orig_width, orig_height = data['imageWidth'], data['imageHeight']\n",
    "        else:\n",
    "            # Fallback: infer dimensions from points\n",
    "            all_x = [p[0] for shape in data['shapes'] for p in shape['points']]\n",
    "            all_y = [p[1] for shape in data['shapes'] for p in shape['points']]\n",
    "            orig_width, orig_height = int(max(all_x)) + 1, int(max(all_y)) + 1\n",
    "\n",
    "        # Scale coordinates\n",
    "        x_scale = img_width / orig_width\n",
    "        y_scale = img_height / orig_height\n",
    "\n",
    "        # Prepare an empty color mask\n",
    "        color_mask = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "\n",
    "        # Draw scaled polygons onto mask\n",
    "        for shape in data['shapes']:\n",
    "            label = shape['label'].lower()  # Case insensitive\n",
    "            if label not in colors:\n",
    "                print(f\"Warning: Unknown label '{label}' in {json_path}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            points = np.array([[int(p[0]*x_scale), int(p[1]*y_scale)] for p in shape['points']], dtype=np.int32)\n",
    "            cv2.fillPoly(color_mask, [points], colors[label])\n",
    "\n",
    "        # Save the generated mask\n",
    "        os.makedirs(os.path.dirname(mask_path), exist_ok=True)\n",
    "        cv2.imwrite(mask_path, color_mask)\n",
    "        print(f\"Generated mask: {mask_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_all_images(image_dir, json_dir, mask_dir, colors):\n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # Process each image\n",
    "    success_count = 0\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for img_file in image_files:\n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "            \n",
    "            # Construct paths\n",
    "            image_path = os.path.join(image_dir, img_file)\n",
    "            json_path = os.path.join(json_dir, f\"{base_name}.json\")\n",
    "            mask_path = os.path.join(mask_dir, f\"{base_name}_mask.png\")\n",
    "            \n",
    "            # Skip if JSON doesn't exist\n",
    "            if not os.path.exists(json_path):\n",
    "                print(f\"Warning: JSON file not found for {img_file}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            futures.append(executor.submit(generate_mask, image_path, json_path, mask_path, colors))\n",
    "        \n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            success_count += 1 if future.result() else 0\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Successfully generated {success_count}/{len(image_files)} masks.\")\n",
    "\n",
    "# Execute the processing\n",
    "process_all_images(image_dir, json_dir, mask_dir, colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
